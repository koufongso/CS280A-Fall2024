<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz"
        crossorigin="anonymous"></script>

    <title>CS280A Fall2024 Project 4</title>

    <style>
        .equal-height {
            height: 400px;
            /* Adjust the height as needed */
            object-fit: cover;
            /* Ensures the images scale without stretching */
        }
    </style>

</head>

<body>
    <nav class="navbar"
        style="--bs-navbar-brand-hover-color: white; --bs-navbar-brand-color: white; background-color: #003262;">
        <div class="container-fluid">
            <span class="navbar-brand mb-0 fs-4 fw-bold">UC Berkeley CS280A: Intro to Computer Vision and Computational
                Photography
                (Fall2024)</span>
        </div>
    </nav>

    <br>
    <div class="container-fluid content">
        <div class="row">
            <div class="col-2">
                <div class="sticky-top">
                    <ul class="nav flex-column fs-5">
                        <li class="nav-item">
                            <a class="nav-link" href="../project1/index.html">Project 1</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../project2/index.html">Project 2</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../project3/index.html">Project 3</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#">Project 4</a>
                            <ul class="nav flex-column fs-6" style="margin-left: 30px;">
                                <li class="nav-item"><a class="nav-link" href="#overview">Overview</a></li>
                                <li class="nav-item"><a class="nav-link" href="#homography">Homography Warping</a></li>
                                <li class="nav-item"><a class="nav-link" href="#cylindrical">Cylindrical Projection</a>
                                <li class="nav-item"><a class="nav-link" href="#mosaicing">Image Mosaicing</a></li>
                                <li class="nav-item"><a class="nav-link" href="#feature">Feature Detection &
                                        Matching</a></li>
                                <li class="nav-item"><a class="nav-link" href="#ransac">Robust Transformation Estimation
                                        Using RANSAC</a></li>
                                <li class="nav-item"><a class="nav-link" href="#auto1">Image Mosaicing with Automated
                                        Transformation Estimation</a></li>
                                <li class="nav-item"><a class="nav-link" href="#auto2">Automated Panorama
                                        Recognition</a></li>
                                <li class="nav-item"><a class="nav-link" href="#last">What have I learned?</a></li>

                            </ul>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../project5/index.html">Project 5</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../final_project/index.html">Final Project</a>
                        </li>
                    </ul>
                </div>
            </div>



            <div class="col-9 content">
                <div id="project2">
                    <h1>Project 4: Image Warping and Mosaicing (<a
                            href="https://github.com/koufongso/CS280A-Fall2024/tree/main/project4"
                            target="_blank">GitHub repo</a>)</h1>
                </div>
                <br>

                <div id="body">
                    <h2 id="overview">Overview</h2>
                    <p>
                        In Project 3, we applied image warping using an affine transformation, which is effective when
                        both images are captured from the same plane without any camera rotation. In this project, we
                        will use a different transformation to warp images taken from various angles but from the same
                        location. This technique is commonly used for image stitching and creating image mosaics. The
                        entire pipeline is illustrated below, with corresponding sections for each component.
                    </p>

                    <figure class="row">
                        <img src="./doc/webpage/overview.jpg" alt="">
                        <figcaption>Overview of the whole pipeline and the corresponding sections.</figcaption>
                    </figure>

                    <br>

                    <h2 id="homography">Homography Warping</h2>
                    <h4>Baic Idea</h4>
                    <p>Figure 1 demonstrates the basic idea of homography warping. Given two images taken from different
                        angles, we want to project them onto an arbitrary image plane to create a single,
                        wide-field-of-view image. The goal is to determine the correct transformation function that
                        accurately projects these images onto the chosen reference image plane. To simplify this
                        process, we use one image plane as the reference, eliminating the need to create a new
                        coordinate system for an image plane. For images with a fixed Center of Projection (COP), this
                        can be achieved using a perspective transformation.</p>

                    <figure>
                        <img src="./doc/webpage/figure1.png" alt="" style="height:40%">
                        <figcaption>Figure 1. Warping of two images taken from different angles.</figcaption>
                    </figure>

                    <h4 id="transformation">Transformation Matrix</h4>
                    <p>Let's take a closer look at the transformation matrix to better understand its structure and how
                        it works. To project a 3D point in space [X, Y, Z] onto an image plane coordinate [x,
                        y], we have:</p>

                    <img src="./doc/webpage/camera_projection.png" style="height:40%">

                    <p>A1 is usually called the camera intrinsic matrix, while T1 is referred to as the camera extrinsic
                        matrix, which transforms 3D points from a fixed coordinate system to the camera coordinate
                        system. T21 is the transformation matrix that transforms coordinates from camera 1's coordinate
                        system to camera 2's coordinate system.</p>

                    <p>We can now examine how various factors affect the complexity of these equations:</p>
                    <ul>
                        <li>Fixed COP: No translation between camera coordinate systems means that T21 can be written as
                            a 3x3 rotational matrix. This can be represented by 3 parameters (Euler angle
                            representation) or 3 unknowns.</li>
                        <li>No zooming: A zooming camera will change fx and fy. If there is no zooming, A2 is the same
                            as A1, resulting in 4 unknowns; otherwise, there will be 6 unknowns.</li>
                    </ul>

                    <p>If we fix the COP and allow zooming, we will have 3 + 6 = 9 unknowns (we can normalize the last
                        entry, leaving us with 8). The expression A2 × T21 × inv(A1) will yield the perspective matrix.
                        If we capture images with a fixed COP and control the rotation (e.g., rotating only horizontally
                        or around the y/vertical axis), we can simplify the matrix T21. We will discuss the rotation
                        model later.</p>

                    <p>It is also possible to warp images onto the same image plane even without a fixed COP; however,
                        this would introduce three additional unknowns (representing translation between the cameras).
                    </p>

                    <br>
                    <h4>Find the Perspective Matrix</h4>
                    <p>We can use correspondences to find the perspective matrix we need. The perspective matrix M is in
                        the form of [[a, b, c], [d, e, f], [g, h, 1]]. Given a point [x1, y1] and the transformed point
                        [x1', y1'], we have:</p>
                    <figure>
                        <img src="./doc/webpage/perspective_matrix.png" alt="" style="height:25%">
                    </figure>

                    <p>where w is the scaling factor and can be interpreted as the depth of the point.</p>

                    <p>We can rewrite this in the form:</p>
                    <figure>
                        <img src="./doc/webpage/solve_perspective_matrix.png" alt="" style="width:40%">
                    </figure>

                    <p>To solve this linear system of equations, we need to have at least 4 pairs of correspondence
                        points. Since we are selecting correspondence points by hand, which is not pinpoint accurate, it
                        is better to have more than 4 pairs of correspondence points to achieve robust results. We can
                        then find the perspective matrix by solving the least squares problem.</p>



                    <h4>Image Rectification Demo</h4>
                    <p>We can also use this method to find the perspective matrix that can be used to rectify an image.
                        We can pick some keypoints on an image and define their coordinates on an image plane. For
                        example, we can select the 4 corners of a window from an image and define their coordinates as
                        the corners of a rectangle. We can then compute the perspective matrix and transform this image
                        into the new image plane with the "rectification effect".</p>


                    <div class="row">
                        <figure class="col">
                            <img src="./doc/rectification/keyboard_origin.jpg" alt="" style="width:50%">
                            <figcaption>Original image. Keypoints are marked in green.</figcaption>
                        </figure>

                        <figure class="col">
                            <img src="./doc/rectification/keyboard_rectified.jpg" alt="" style="width:50%">
                            <figcaption>Rectified image. Keypoints after transformation are marked in green. We can see
                                that the rectified image appears as if we are looking downward.</figcaption>
                        </figure>
                    </div>

                    <div class="row">
                        <figure class="col">
                            <img src="./doc/rectification/book_origin.jpg" alt="" style="width:50%">
                            <figcaption>Original image. Keypoints are marked in green.</figcaption>
                        </figure>

                        <figure class="col">
                            <img src="./doc/rectification/book_rectified.jpg" alt="" style="width:50%">
                            <figcaption>Rectified image. Keypoints after transformation are marked in green.
                            </figcaption>
                        </figure>
                    </div>

                    <div class="row">
                        <figure class="col">
                            <img src="./doc/rectification/building_origin.jpg" alt="" style="width:50%">
                            <figcaption>Original image. Keypoints are marked in green.</figcaption>
                        </figure>

                        <figure class="col">
                            <img src="./doc/rectification/building_rectified_crop.jpg" alt="" style="width:50%">
                            <figcaption>Rectified image. The keypoints after transformation are marked in green. The
                                actual image is quite large, so it has been cropped to fit the webpage.</figcaption>
                        </figure>
                    </div>

                    <br>
                    <br>

                    <h4>Special Case: Rotational Model</h4>
                    <p>As we mentioned in the previous <a href="#transformation">section</a>, if we only rotate the
                        camera, it is possible to simplify the transformation matrix. Additionally, it is possible to
                        back-compute the camera intrinsic parameters from the transformation matrix.</p>

                    <p>If we take images using a fixed COP, with no zooming, and only rotate the camera horizontally
                        (i.e., rotate along the camera's y/vertical axis), we can understand the structure of the
                        transformation matrix.</p>
                    <img src="./doc/webpage/rotation_matrix.png" alt="" style="width:50%">

                    <p>The result of using the rotational matrix model to perform image mosaicing is shown <a
                            href="#rotation_demo">here</a>.</p>
                    <br>

                    <h2 id="cylindrical">Cylindrical Projection</h2>
                    <h4>Basic Idea</h4>
                    <p>Planar projection is not the only option here; we can also project images onto a cylindrical
                        surface. Figure 2 illustrates cylindrical projection. Note that the x/horizontal axis does not
                        follow the curve of the surface.</p>

                    <figure>
                        <img src="./doc/webpage/figure2.png" alt="" style="width:50%">
                        <figcaption>Figure 2. Warping of images onto a cylindrical surface.</figcaption>
                    </figure>

                    <h4>Mapping between Planar and Cylindrical Surface</h4>
                    <p>We can map the coordinates between the planar and cylindrical surfaces as illustrated below. \( f
                        \) is the focal length of the camera, and \( r \) is the radius of the cylinder, which is a
                        tunable parameter we decide.</p>

                    <p><span style="color: red;">Important</span>: To simplify the formula, we shift the coordinate
                        system so that the center of the image is located at the origin. Therefore, we need to shift it
                        back to the regular image coordinate system after computing (i.e., the top-left corner pixel is
                        located at the origin).</p>

                    <img src="./doc/webpage/figure3.png" alt="" style="width:70%">


                    <p>We can derive the formula for forward and inverse mapping between the planar and cylindrical
                        surfaces:</p>
                    <img src="./doc/webpage/map2cylinder.png" alt="" style="height:40%">

                    <p>The focal length of the camera can be computed as:</p>
                    <img src="./doc/webpage/focal.png" alt="" style="height:20%">
                    <p>HFOV: horizontal field of view.</p>


                    <h4>Cylindrical Projection Demo</h4>
                    <p>Below are some sample images projected onto the cylindrical surface; here, I set the radius equal
                        to the focal length \( f \). I am using a camera with an HFOV of 65.6 degrees. Notice the
                        distortion effects on the images from cylindrical projection (especially the bending of the
                        building edges).</p>

                    <div class="row">
                        <figure class="col">
                            <img src="./images/test1/01.jpg" class="img-fluid equal-height">
                            <p>Planar image</p>
                        </figure>

                        <figure class="col">
                            <img src="./doc/test1/blend_cylinrical/01_c.jpg" class="img-fluid equal-height">
                            <p>Cylindrical image</p>
                        </figure>
                    </div>


                    <div class="row">
                        <figure class="col">
                            <img src="./images/test1/02.jpg" class="img-fluid equal-height">
                            <p>Planar image</p>
                        </figure>

                        <figure class="col">
                            <img src="./doc/test1/blend_cylinrical/02_c.jpg" class="img-fluid equal-height">
                            <p>Cylindrical image</p>
                        </figure>
                    </div>


                    <div class="row">
                        <figure class="col">
                            <img src="./images/test1/03.jpg" class="img-fluid equal-height">
                            <p>Planar image</p>
                        </figure>

                        <figure class="col">
                            <img src="./doc/test1/blend_cylinrical/03_c.jpg" class="img-fluid equal-height">
                            <p>Cylindrical image</p>
                        </figure>
                    </div>


                    <div class="row">
                        <figure class="col">
                            <img src="./images/test1/04.jpg" class="img-fluid equal-height">
                            <p>Planar image</p>
                        </figure>

                        <figure class="col">
                            <img src="./doc/test1/blend_cylinrical/04_c.jpg" class="img-fluid equal-height">
                            <p>Cylindrical image</p>
                        </figure>
                    </div>


                    <h2 id="mosaicing">Image Mosaicing</h2>
                    <h4>Planar Image Mosaicing</h4>
                    <p>There are two ways to do image mosaicing. One method is to warp two images and produce a new
                        image, then repeat this step to recursively generate the final mosaic. The other method is to
                        warp all images onto one common reference plane and blend them. Here, I chose the "one-shot"
                        method. I will skip the technical details and just describe the general steps.</p>

                    <ol>
                        <li><b>Choose the reference image</b>: Select one reference image and use it as the reference
                            image plane.</li>
                        <li><b>Warp all images</b>: Warp all images to this plane using the perspective matrix. Also,
                            keep track of the corner coordinates of the warped images. We need to compute the overlap
                            area later, and since the perspective transformation is a linear operation, the warped
                            image's shape is a polygon defined by its four corners. Therefore, I also keep track of
                            these corners' coordinates.</li>
                        <li><b>Blending</b>: Manually choose two images with overlap to blend. I tested the average
                            blending method and multi-band (Laplacian blending from Project 2) blending method here.
                            <ul>
                                <li><b>Averaging blending</b>: I_blend(i,j) = 0.5×I1(i,j)+0.5 ×I2(i,j) for (i,j) in
                                    the overlap area.</li>
                                <li><b>Multi-band blending</b>: To prepare the mask, compute the overlap area. For each
                                    pixel in that area, we compare the distances between this pixel and the two images'
                                    edges. For example, for a given pixel (i,j), the distances to the nearest edges
                                    of image 1 and image 2 are d1 and d2, respectively. The mask for image 1,
                                    w(i,j), is d1/(d1 + d2). This means that the closer the pixel is
                                    to image 1's edge, the less weight is given to image 1's pixel. After obtaining the
                                    mask, use the Laplacian blending method with the Laplacian stack of images and the
                                    Gaussian stack of the mask.</li>
                            </ul>
                        </li>
                    </ol>

                    <h4>Planar Mosaic Demo 1: Average Blending VS Multi-band Blending</h4>
                    <div class="row">
                        <figure class="col">
                            <img src="./images/test1/01.jpg" class="img-fluid equal-height">
                            <p>Planar image 1</p>
                        </figure>
                        <figure class="col">
                            <img src="./images/test1/02.jpg" class="img-fluid equal-height">
                            <p>Planar image 2</p>
                        </figure>
                    </div>
                    <div class="row">
                        <figure class="col">
                            <img src="./doc/test1/blender_two/warp_image01.jpg" class="img-fluid equal-height">
                            <p>Warpped image 1</p>
                        </figure>
                        <figure class="col">
                            <img src="./doc/test1/blender_two/warp_image02.jpg" class="img-fluid equal-height">
                            <p>Warpped image 2</p>
                        </figure>
                    </div>

                    <div class="row">
                        <figure class="col">
                            <img src="./doc/test1/blender_two/im_blend_simple.jpg" class="img-fluid equal-height">
                            <p>Averaging blending</p>
                        </figure>
                        <figure class="col">
                            <img src="./doc/test1/blender_two/im_blend_laplacian.jpg" class="img-fluid equal-height">
                            <p>Multi-band blending</p>
                        </figure>
                    </div>

                    <div class="row">
                        <figure class="col">
                            <img src="./doc/test1/blender_two/alpha.jpg" class="img-fluid equal-height">
                            <p>Mask used by image 1 in multi-band blending</p>
                        </figure>
                    </div>
                    <br>
                    <h4>Planar Mosaic Demo 1</h4>
                    <div class="row">
                        <figure class="col">
                            <img src="./images/test1/01.jpg" class="img-fluid equal-height">
                            <p>Planar image 1</p>
                        </figure>
                        <figure class="col">
                            <img src="./images/test1/02.jpg" class="img-fluid equal-height">
                            <p>Planar image 2</p>
                        </figure>
                    </div>
                    <div class="row">
                        <figure class="col">
                            <img src="./images/test1/03.jpg" class="img-fluid equal-height">
                            <p>Warpped image 3</p>
                        </figure>
                        <figure class="col">
                            <img src="./images/test1/04.jpg" class="img-fluid equal-height">
                            <p>Warpped image 4</p>
                        </figure>
                    </div>

                    <div class="row">
                        <figure class="col">
                            <img src="./doc/test1/blend_homography/blend_multiband.jpg" class="img-fluid">
                            <p>Multi-band blending</p>
                        </figure>
                    </div>


                    <h4>Planar Mosaic Demo 2</h4>
                    <div class="row">
                        <figure class="col">
                            <img src="./images/test2/01.jpg" class="img-fluid equal-height">
                            <p>Planar image 1</p>
                        </figure>
                        <figure class="col">
                            <img src="./images/test2/02.jpg" class="img-fluid equal-height">
                            <p>Planar image 2</p>
                        </figure>
                    </div>
                    <div class="row">
                        <figure class="col">
                            <img src="./images/test2/03.jpg" class="img-fluid equal-height">
                            <p>Planar image 3</p>
                        </figure>
                        <figure class="col">
                            <img src="./images/test2/04.jpg" class="img-fluid equal-height">
                            <p>Planar image 4</p>
                        </figure>
                    </div>


                    <div class="row">
                        <figure class="col">
                            <img src="./doc/test2/blend_h.jpg" class="img-fluid">
                            <p>Multi-band blending</p>
                        </figure>
                    </div>


                    <h4>Planar Mosaic Demo 3</h4>
                    <div class="row">
                        <figure class="col">
                            <img src="./images/test3/01.jpg" class="img-fluid equal-height">
                            <p>Planar image 1</p>
                        </figure>
                        <figure class="col">
                            <img src="./images/test3/02.jpg" class="img-fluid equal-height">
                            <p>Planar image 2</p>
                        </figure>
                        <figure class="col">
                            <img src="./images/test3/03.jpg" class="img-fluid equal-height">
                            <p>Planar image 3</p>
                        </figure>
                    </div>


                    <div class="row">
                        <figure class="col">
                            <img src="./doc/test3/blend_h.jpg" class="img-fluid">
                            <p>Multi-band blending</p>
                        </figure>
                    </div>

                    <h4 id="rotation_demo">Rotational Model Mosaicing Demo</h4>
                    <p>We use the rotational model to warp the image. Noticed that since we assume pure rotation,
                        the
                        image verticle edges are stricly verticle, whcih is differnt than the one using perspective
                        matrix.</p>
                    <div class="row">
                        <figure class="col">
                            <img src="./doc/test1/blend_rotation/blend_r.jpg" class="img-fluid">
                            <p>Multi-band blending using rotational model</p>
                        </figure>
                    </div>



                    <h4>Cylindrical Mosaicing Demo</h4>
                    <p>When using cylindrical coordinate, we can just simply use a trasnlation matrix rather than
                        perspective matrix to warp the image. The only diffence here is we need to warp the
                        correspondence keypointsthe to the cylindrical coordiantes first and then compute the
                        translation matrix and use it to warp the images, the rest of the workflow is the same.</p>

                    <div class="row">
                        <figure class="col">
                            <img src="./doc/test1//blend_cylinrical/blend__multiband_c.jpg" class="img-fluid">
                            <p>Multi-band blending with cylindrical projection.</p>
                        </figure>
                    </div>

                    <p>You can see that compare to the planar mosaic, the cylindrical mosaic looks better and don't
                        have
                        the strong distorsion effect when the image is away from the reference image center. You can
                        see
                        that the warpped image 4 in the plannar surface is largely distorted since it is relatively
                        far
                        away form the image 2, which is our reference image. But we won't see such strong distrosion
                        in
                        the cylindrical surface. </p>

                    <h4>360 Panorama Demo</h4>
                    <p>We can use cylindrical projection to generate a 360° panorama. Ideally, the first and last images
                        should "close the loop," but in our case, they are not perfectly aligned due to numerical errors
                        and the images not having a perfectly fixed COP. Instead of aligning two
                        images at a time, a better approach might be to align all images using global optimization. For
                        now, we will leave it as is.</p>


                    <div id="carouselExample" class="carousel slide" data-bs-ride="carousel">
                        <div class="carousel-inner">
                            <div class="carousel-item active">
                                <div class="row">
                                    <div class="col-4">
                                        <img src="./images/360panorama/01.jpg" class="d-block w-100" alt="Image 1">
                                    </div>
                                    <div class="col-4">
                                        <img src="./images/360panorama/02.jpg" class="d-block w-100" alt="Image 2">
                                    </div>
                                    <div class="col-4">
                                        <img src="./images/360panorama/03.jpg" class="d-block w-100" alt="Image 3">
                                    </div>
                                </div>
                            </div>
                            <div class="carousel-item">
                                <div class="row">
                                    <div class="col-4">
                                        <img src="./images/360panorama/04.jpg" class="d-block w-100" alt="Image 4">
                                    </div>
                                    <div class="col-4">
                                        <img src="./images/360panorama/05.jpg" class="d-block w-100" alt="Image 5">
                                    </div>
                                    <div class="col-4">
                                        <img src="./images/360panorama/06.jpg" class="d-block w-100" alt="Image 6">
                                    </div>
                                </div>
                            </div>
                            <div class="carousel-item">
                                <div class="row">
                                    <div class="col-4">
                                        <img src="./images/360panorama/07.jpg" class="d-block w-100" alt="Image 7">
                                    </div>
                                    <div class="col-4">
                                        <img src="./images/360panorama/08.jpg" class="d-block w-100" alt="Image 8">
                                    </div>
                                    <div class="col-4">
                                        <img src="./images/360panorama/09.jpg" class="d-block w-100" alt="Image 9">
                                    </div>
                                </div>
                            </div>
                            <div class="carousel-item">
                                <div class="row">
                                    <div class="col-4">
                                        <img src="./images/360panorama/10.jpg" class="d-block w-100" alt="Image 10">
                                    </div>
                                    <div class="col-4">
                                        <img src="./images/360panorama/11.jpg" class="d-block w-100" alt="Image 11">
                                    </div>
                                    <div class="col-4">
                                        <img src="./images/360panorama/12.jpg" class="d-block w-100" alt="Image 12">
                                    </div>
                                </div>
                            </div>
                        </div>

                        <!-- Carousel Controls -->
                        <button class="carousel-control-prev" type="button" data-bs-target="#carouselExample"
                            data-bs-slide="prev">
                            <span class="carousel-control-prev-icon" aria-hidden="true"></span>
                            <span class="visually-hidden">Previous</span>
                        </button>
                        <button class="carousel-control-next" type="button" data-bs-target="#carouselExample"
                            data-bs-slide="next">
                            <span class="carousel-control-next-icon" aria-hidden="true"></span>
                            <span class="visually-hidden">Next</span>
                        </button>
                    </div>
                    <p>12 images used for 360 panorama.</p>


                    <br>
                    <br>
                    <div class="row">
                        <figure class="col">
                            <img src="./doc/360panorama/360panorama.jpg" class="img-fluid">
                            <p>360 Panorama using multi-band blending. </p>
                        </figure>
                    </div>

                    <h4>360 Panorama Demo</h4>
                    <p>We can use cylindrical projection to generate a 360° panorama. Ideally, the first and last images
                        should "close the loop," but in our case, they are not perfectly aligned due to numerical errors
                        and the images not having a perfectly fixed COP. Instead of aligning two
                        images at a time, a better approach might be to align all images using global optimization. For
                        now, we will leave it as is.</p>



                    <h2 id="feature">Feature Detection & Matching</h2>

                    <p>
                        So far, we have been finding correspondences manually, which can be time-consuming and prone to
                        errors and bias. To automate this process, we will develop the following three components in
                        this section:
                    </p>
                    <ul>
                        <li><b>Keypoint Detector</b>: Detects keypoints or interesting points in an image.</li>
                        <li><b>Descriptor Extractor</b>: Computes a descriptor that encodes information about the area
                            surrounding each keypoint.</li>
                        <li><b>Feature Matcher</b>: Matches keypoints between two images using their descriptors.</li>
                    </ul>
                    <p>
                        We are following the methodology from "Multi-Image Matching using Multi-Scale Oriented Patches"
                        (Matthew Brown et al., 2005) to implement these components.
                    </p>

                    <h4>Keypoint Detector</h4>
                    <p>
                        We begin by extracting keypoints (or interesting points) using the <b>Harris Corner
                            Detector</b>. To downsample the keypoints, it’s common to select the top-k keypoints with
                        the highest "Harris corner strength". However, this approach may lead to clustering of
                        keypoints, which is not ideal for image stitching. Since we don’t know which parts of the image
                        will be matched with others, it’s preferable to have keypoints distributed more uniformly across
                        the image.
                    </p>
                    <p>
                        To achieve this, we will use <b>Adaptive Non-Maximum Suppression (ANMS)</b>, introduced by
                        Matthew Brown. The idea of ANMS is straightforward: it sorts keypoints based on a specific
                        radius, which represents the distance to the nearest keypoint that meets a certain "Harris
                        corner strength" criterion. Formally, we define the radius r_i = min(|x_i - x_j|) where
                        f(x_i) < c*f(x_j), with c being a constant. </p>

                            <p>
                                The figures below illustrate the difference in keypoint detection with and without ANMS.
                                You can see that keypoints filtered by the ANMS method are more evenly distributed
                                across the image compared to those selected without using ANMS.
                            </p>



                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/features/500_regular.jpg" class="img-fluid">
                                    <p>Top 500 keypoints with the highest Harris corner strength (marked in green).</p>
                                </figure>

                                <figure class="col">
                                    <img src="./doc/features/500_ANMS.jpg" class="img-fluid">
                                    <p>Top 500 keypoints filtered by ANMS (marked in green).</p>
                                </figure>
                            </div>

                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/features/test2_500_regular.jpg" class="img-fluid">
                                    <p>Top 500 keypoints with the highest Harris corner strength (marked in green).</p>
                                </figure>

                                <figure class="col">
                                    <img src="./doc/features/test2_500_ANMS.jpg" class="img-fluid">
                                    <p>Top 500 keypoints filtered by ANMS (marked in green).</p>
                                </figure>
                            </div>

                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/features/test3_500_regular.jpg" class="img-fluid">
                                    <p>Top 500 keypoints with the highest Harris corner strength (marked in green).</p>
                                </figure>

                                <figure class="col">
                                    <img src="./doc/features/test3_500_ANMS.jpg" class="img-fluid">
                                    <p>Top 500 keypoints filtered by ANMS (marked in green).</p>
                                </figure>
                            </div>


                            <h4>(Rotation-Invariant) Descriptor Extractor</h4>
                            <p>
                                After detecting the keypoints, we use an 8x8 patching method to form a 64-byte vector
                                that encodes pixel information around each keypoint.
                            </p>
                            <p>
                                First, we determine the orientation angle of each keypoint by computing the gradient at
                                that location. Then, we sample an 8x8 patch around the keypoint. To reduce aliasing, we
                                sample every 5 pixels, which requires sampling from a 40x40 area around the keypoint.
                            </p>


                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/features/descriptor_patch.jpg" class="img-fluid">
                                    <p>Orientation/gradient vector of a keypoint (marked with a blue arrow) and its 64
                                        sampling points (marked with green dots).</p>
                                </figure>
                            </div>


                            <h4>Feature Matcher</h4>
                            <p>We can treat each descriptor as a 64-byte vector, therefore, we can use the L2 norm to
                                compute the "distance" between each pair of descriptors. A naive approach would use
                                brute-force computation to evaluate all possible combinations and find the top-k pairs.
                                A more efficient approach is to use a KD-tree structure. In this project,
                                <code>scipy.spatial.cKDTree</code> was utilized to find the k-nearest neighborhood
                                descriptors for a given query descriptor.
                            </p>
                            <p>Given a query descriptor, after obtaining the top-k descriptors with the lowest
                                distances, we would normally take the first one as the match. However, even the top
                                matching result is not always correct. To reduce this possibility, we apply <b>Lowe's
                                    ratio test</b> to find the "good" matches. The idea is to compute the ratio between
                                the 1st and 2nd matching results. If the ratio is higher than a predefined threshold,
                                the 1st match is considered "good." In addition, I also set a maximum distance
                                threshold, ensuring that any "good" match does not exceed this limit.</p>
                            <p>The figure below shows some examples of "good" matches between images. You might notice
                                that some matches look incorrect even though they have small descriptor distances. This
                                can occur when a scene contains multiple similar features, such as the corners of
                                windows or doors with similar styles. This can be problematic if we directly use them
                                to solve our transformation/warping matrix. We will address this problem in the next
                                section.</p>

                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test1/matches.jpg" class="img-fluid">
                                    <p>Image pair 1. Matched features are connected by red lines.</p>
                                </figure>
                            </div>
                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test2/matches.jpg" class="img-fluid">
                                    <p>Image pair 2. Matched features are connected by red lines.</p>
                                </figure>
                            </div>
                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test3/matches.jpg" class="img-fluid">
                                    <p>Image pair 3. Matched features are connected by red lines.</p>
                                </figure>
                            </div>

                            <h4>Multi-scale Processing</h4>
                            <p>We can create a Gaussian pyramid of the input images, and the processes described above
                                still apply. The only thing to pay attention to is that <b>the detected keypoints need
                                    to
                                    be scaled back to the original size when we use them to compute the transformation
                                    matrix</b>, as we are stitching the original image.</p>
                            <p>Below are some images illustrating the multi-scale process. Outliers (incorrect matches)
                                and inliers (correct matches) are identified through RANSAC estimation, which we will
                                discuss in the next section. As the number of pyramid levels increases, we observe that
                                the number of outliers decreases.</p>

                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test1/matches_ransac.jpg" class="img-fluid">
                                    <p>Image pair at pyramid level 0 (original images). Inliers and outliers are identified using the RANSAC method. Outlier
                                        matches are marked in red, while inlier matches are marked in green.</p>
                                </figure>
                            </div>

                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test1/matches_lv1.jpg" class="img-fluid">
                                    <p>Image pair at pyramid level 1 (blurred with a (5,5) Gaussian window and halved in
                                        size). Inliers and outliers are identified using the RANSAC method. Outlier
                                        matches are marked in red, while inlier matches are marked in green.</p>
                                </figure>
                            </div>

                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test1/matches_lv2.jpg" class="img-fluid">
                                    <p>Image pair at pyramid level 2 (blurred with a (5,5) Gaussian window and halved in
                                        size). Inliers and outliers are identified using the RANSAC method. Outlier
                                        matches are marked in red, while inlier matches are marked in green.</p>
                                </figure>
                            </div>


                            <h2 id="ransac">Robust Transformation Estimation Using RANSAC</h2>
                            <p>We can now automatically find correspondences from image pairs using the feature-based
                                method. However, this method may still yield incorrect matches due to similar features
                                present in the scene. Therefore, we will use Random Sample Consensus (RANSAC) to enhance
                                the robustness of the transformation estimation against these outliers.</p>
                            <p>The idea behind RANSAC is to randomly sample four points (the minimum number of points
                                required to solve the perspective transformation matrix), compute the matrix, and track
                                the number of inliers/outliers using the given combination. After repeating this
                                process N times (e.g. N=1000), we select the combination with the largest number of
                                inliers, or the
                                smallest number of outliers, then solve the perspective matrix using all the inliers.
                            </p>
                            <p>The figure below illustrates the inliers and outliers of the matched pairs.</p>

                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test1/matches_ransac.jpg" class="img-fluid">
                                    <p>Image pair 1. Outliner matches are marked in red, inliner matches are marked in
                                        green.</p>
                                </figure>
                            </div>
                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test2/matches_ransac.jpg" class="img-fluid">
                                    <p>Image pair 2.Outliner matches are marked in red, inliner matches are marked in
                                        green.</p>
                                </figure>
                            </div>
                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test3/matches_ransac.jpg" class="img-fluid">
                                    <p>Image pair 3. Outliner matches are marked in red, inliner matches are marked in
                                        green.</p>
                                </figure>
                            </div>


                            <h2 id="auto1">Image Mosaicing with Automated Transformation Estimation</h2>
                            <p>We can now automate the image mosaicing process by using a feature-based method to obtain
                                correspondences and compute the transformation matrix. Below are the mosaics created
                                using manually found correspondences and automatically detected correspondences. You can
                                see that these results are visually identical. Note that both processes still require
                                manually defining the blending order between images, which we will address in the next
                                section to create a fully automated image mosaicing process.</p>

                            <h4>Demo 1</h4>
                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test1/blend_homography/blend_multiband.jpg" class="img-fluid">
                                    <p>Mosaicing using manually labeled correspondences.</p>
                                </figure>
                                <figure class="col">
                                    <img src="./doc/test1/blend_homography/blend__multiband_fm_h.jpg" class="img-fluid">
                                    <p>Mosaicing using feature-based detected correspondences.</p>
                                </figure>
                            </div>

                            <h4>Demo 2</h4>
                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test2/blend_h.jpg" class="img-fluid">
                                    <p>Mosaicing using manually labeled correspondences.</p>
                                </figure>
                                <figure class="col">
                                    <img src="./doc/test2/blend__multiband_fm_h.jpg" class="img-fluid">
                                    <p>Mosaicing using feature-based detected correspondences.</p>
                                </figure>
                            </div>

                            <h4>Demo 3</h4>
                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test3/blend_h.jpg" class="img-fluid">
                                    <p>Mosaicing using manually labeled correspondences.</p>
                                </figure>
                                <figure class="col">
                                    <img src="./doc/test3/blend__multiband_fm_h.jpg" class="img-fluid">
                                    <p>Mosaicing using feature-based detected correspondences.</p>
                                </figure>
                            </div>

                            <h2 id="auto2">Automated Panorama Recognition</h2>
                            <p>Although we can detect correspondences automatically and use them for image mosaicing, we
                                still need to ensure that the input images can be stitched together and define how the
                                images should be blended. In this section, we will automate this manual input so that we
                                can simply provide a set of images, and the system will automatically recognize images
                                with overlaps and create a mosaic or panorama.</p>
                            <p>To achieve this, we need to do the following:</p>
                            <ol>
                                <li>Detect image keypoints and compute descriptors. (Done)</li>
                                <li>Find an image sequence that connects image i to image j. Use this sequence to
                                    compute the transformation matrix from image i to image j. (Need to be solved)</li>
                                <li>Warp image i using the transformation matrix to align with image j. (Done)</li>
                                <li>Blend all images that have been warped to the common image plane. (Done, but needs
                                    modification)</li>
                            </ol>
                            <p>Step 1 can be achieved using the feature-based method described in the previous section.
                                The key challenge is step 2, which involves finding, if it exists, a sequence/path that
                                connects image i to image j (in this case, our reference image). This will be solved
                                using a graph data structure.</p> <br>
                            <h4>Image Graph</h4>
                            <p>We can find a path from image i to image j using a graph data structure. First, we will
                                iterate through all possible combinations of image pairs to form the
                                connectivity/adjacency matrix. Image i and image j are connected if they have a minimum
                                of <b>k</b> matched features. A graph might look something like the figure below, where
                                the link number represents the number of matched features and the node number is the
                                image index. We can now use the graph to find the path from image i to image j.</p>
                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/webpage/graph.jpg" class="img-fluid" style="height: 300px;">
                                    <p>Graph example.</p>
                                </figure>
                            </div>

                            <h4>Find the Path</h4>
                            <p>Normally, we can use the <b>Breadth-First Search (BFS)</b> algorithm to find the shortest
                                path from image i to image j, if it exists. However, during experiments, it became
                                evident that even though two images are distant from each other, they may still have
                                enough matched features to meet the threshold <b>k</b>. The resulting
                                transformation matrix might not align the images properly. For example, in the graph
                                shown above, if we set image 3 as the reference path, using the shortest path method may
                                reveal a direct transformation from image 1 to image 3. Even though the transformed
                                image 1 can align perfectly with image 3, it may not align well with image 2.</p>
                            <p>Increasing the threshold is not a scalable approach, as it can sever weakly connected but
                                critical image links (e.g., using a threshold of 25 may cut the link between image 2 and
                                image 3, which are both part of the mosaic).</p>

                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/auto/test2_match0-2.jpg" class="img-fluid">
                                </figure>
                                <figure class="col">
                                    <img src="./doc/auto/test2_wrong.jpg" class="img-fluid">

                                </figure>
                                <p>Illustration of the shortest path issue. Eventhough image 1-3 has good matches
                                    (left), the
                                    error still casuing misalignment between image 1 and 2 (right) since all the matches
                                    betwen
                                    1-3 are concentrate on a small area.</p>
                            </div>

                            <p>A better approach is <b>to use a modified version of BFS to track the number of inlier
                                    matches along the path and to keep the path that has the highest minimum
                                    matches.</b> In the previous example, the path 1-2 has a minimum match of 12, while
                                the path 1-2-3 has a minimum match of 20. Hence, the new algorithm will choose the 1-2-3
                                path, which is more reliable. Since we are not finding the path with cumulative maximum
                                matches (like Dijkstra's algorithm), we can also avoid paths like 2-1-3.</p>



                            <h4>Choose the Reference Image</h4>
                            <p>Now we can pick a reference image. The simplest way would be to select the node with the
                                most links. However, we may encounter scenarios where an image near the edge also has
                                matches with distant images. This can result in the warped area requiring a large amount
                                of memory to allocate these images, and the multi-band Laplacian blending can exacerbate
                                this situation. To reduce the likelihood of encountering this issue, we decided to use
                                the node with the largest amount of matches as the reference node (in the previous
                                example, this would be image 2).</p>

                            <h4>Modified One-shot Multi-band Blending</h4>
                            <p>In my previous implementation, I used one-shot blending to warp all images on the same
                                image plane first and then blend every two overlap images manually. More specifically, I
                                first sum all the pixel values, and then blend the overlap area between two image pair
                                defined manually. This workflow becomes complicated in the automated system. Therefore,
                                I implemented a modified version. The idea is similar, for each image, I define a
                                "distance mask" where each pixel has a value to the nearst edge of the warped image.
                                Then I will compute the alpha mask for blending for each image, where the mask value at
                                pixel (i,j) is decided by distances across all the images distance masks at the same
                                localtion.
                            </p>


                            <h4>Auto panorama demo 1</h4>
                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test1/blend_homography/blend_multiband.jpg" class="img-fluid">
                                    <p>Manual stitching</p>
                                </figure>

                                <figure class="col">
                                    <img src="./doc/auto/test1.jpg" class="img-fluid">
                                    <p>Auto stitching.</p>
                                </figure>
                            </div>

                            <h4>Auto panorama demo 2</h4>
                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test2/blend_h.jpg" class="img-fluid">
                                    <p>Manual stitching</p>
                                </figure>

                                <figure class="col">
                                    <img src="./doc/auto/test2.jpg" class="img-fluid">
                                    <p>Auto stitching</p>
                                </figure>
                            </div>

                            <h4>Auto panorama demo 3</h4>
                            <div class="row">
                                <figure class="col">
                                    <img src="./doc/test3/blend_h.jpg" class="img-fluid">
                                    <p>Manual stitching</p>
                                </figure>

                                <figure class="col">
                                    <img src="./doc/auto/test3.jpg" class="img-fluid">
                                    <p>Auto stitching</p>
                                </figure>
                            </div>


                            <h2 id="last">What have I learned? </h2>
                            <p>This project was a large but interesting project, during which I used the images I
                                collected and played with the things I learned from the class. Even though the concept
                                is not difficult, it still takes a lot of time and effort to overcome many technical
                                challenges, like which data structure is suitable for the warping application and how to
                                manipulate the images in an efficient way (e.g., for loops vs. vectorization). And it is
                                pretty cool to see my code work at the end after intense debugging.</p>

                </div>

                <nav class="navbar"
                    style="--bs-navbar-brand-hover-color: white; --bs-navbar-brand-color: white; background-color: #003262;">
                    <div class="container-fluid">
                        <span class="navbar-brand mb-0 fs-4 fw-bold"> </span>
                    </div>
                </nav>
            </div>


        </div>
    </div>



</body>

</html>